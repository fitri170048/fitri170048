{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Modelling/","text":"Modelling Clusterring Clusterring merupakan tugas deskripsi yang digunakan untuk himpunan terbatas untuk mendeskripsikan data yang diolah Tujuan metode ini adalah mengelompokkan data yang homogen/sejenis sehingga data yang berada di cluster yang sama mempunyai banyak kesamaan dibanding dengan data yang ada di cluster yang berbeda. Source code untuk clustering : import library pandas dan inisialisasikan menjadi pd import pandas as pd import library numpy dan inisialisasikan menjadi np import numpy as np dari library math import function log from math import log dari library sklearn fitur ekstrasi teks import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer fungsi untuk menghitung idf def idf(doc, wordBank): #hitung jumlah doc N = len(doc.index) #buat dataframe dengan header word dan idf result = pd.DataFrame(columns=['word', 'idf']) #untuk setiap kata pada wordBank lakukan..... for index, word in wordBank.iterrows(): #hitung jumlah doc yang mengandung kata word['words'] dft = np.sum(doc.str.contains(word['kata'])) #hitung inverse document frequency smooth idft = log(N / (dft + 1), 10) #tambahkan idf untuk setiap kata pada data frame result = result.append(pd.Series([word['kata'], idft], index=['word', 'idf']), ignore_index=True) #return variable result return result fungsi untuk menghitung tf``def tf(doc, wordBank): #split kata berdasarkan spasi wordList = doc.str.split(' ') #hitung jumlah kata pada setiap doc maxFt = [len(s) for s in wordList] #buat DataFrame kosong untuk menyimpan hasil perhitungan Tf result = pd.DataFrame() #untuk setiap word dalam wordbank lakukan .... for index, word in wordBank.iterrows(): #hitung frekuensi kata untuk setiap doc ft = np.add([s.count(word['kata']) for s in wordList], 0) #tf log normalization ftd = 1 + np.log10(ft+1e-15) #tambahkan hasil perhitungan tf kedalam DataFrame result = result.append(pd.Series(ftd), ignore_index=True) #mengganti -inf dengan 0 result = result.replace(-np.inf, 0) #return variable result return result def tfIdf(tf, idf): #buat DataFrame kosong untuk menyimpan hasil perhitungan Tf-Idf result = pd.DataFrame() #untuk setiap tf for i in tf: #tf idf untuk document term weighting tf * idf tfIdf = tf[i] * idf['idf'] #tambahkan hasil perhitungan tf idf kedalam DataFrame result = result.append(pd.Series(tfIdf), ignore_index=True) #return variable result return result membaca file dataTest.csv menggunakan pandas doc = pd.read_csv('dataTest.csv', encoding='utf-8') membaca file wordBank.csv wordBank.csv berisi kata-kata penting atau keywords yang dianggap relevan dengan topik dalam sebuah kasus vocabulary = pd.read_csv('wordBank.csv', encoding='utf-8') ubah seluruh objek menjadi str/string doc = doc['sentences'].str.lower() mengonversi teks menjadi vektor jumlah kata count = CountVectorizer() bag = count.fit_transform(doc) print(bag) melihat fitur kata a=count.get_feature_names() print(a) hitung nilai IDF resultIDF = idf(doc, vocabulary) print('\\n'+'Hasil perhitungan nilai IDF') print(resultIDF) cetak hasil IDF resultIDF.to_csv(r'nilaiIDF.csv', index = False) print('Output saved in \"nilaiIDF.csv\"') hitung nilai TF resultTF = tf(doc, vocabulary) print('\\n'+'Hasil perhitungan nilai TF') print(resultTF) cetak hasil TF resultTF.to_csv(r'nilaiTF.csv', index = False) print('Output saved in \"nilaiTF.csv\"') hitung nilai Tf-Idf resultTfIdf = tfIdf(resultTF, resultIDF) print('\\n'+'Hasil perhitungan nilai Tf-Idf') print(resultTfIdf) cetak hasil Tf-Idf resultTfIdf.to_csv(r'VSM(TF-IDF).csv', index = False) print('Output saved in \"VSM(TF-IDF).csv\"') Classification Classification adalah metode yang paling umum pada data mining. Persoalan bisnis sperti Churn Analysis, dan Risk Management biasanya melibatkan metode Classification.Classification adalah tindakan untuk memberikan kelompok pada setiap keadaan. Setiap keadaan berisi sekelompok atribut, salah satunya adalah class attribute. Metode ini butuh untuk menemukan sebuah model yang dapat menjelaskan class attribute itu sebagai fungsi dari input attribute. Referensi https://glints.com/id/lowongan/web-crawling-adalah/#.YMFsiKgzbIU","title":"Modelling"},{"location":"Modelling/#modelling","text":"","title":"Modelling"},{"location":"Modelling/#clusterring","text":"Clusterring merupakan tugas deskripsi yang digunakan untuk himpunan terbatas untuk mendeskripsikan data yang diolah Tujuan metode ini adalah mengelompokkan data yang homogen/sejenis sehingga data yang berada di cluster yang sama mempunyai banyak kesamaan dibanding dengan data yang ada di cluster yang berbeda. Source code untuk clustering : import library pandas dan inisialisasikan menjadi pd import pandas as pd import library numpy dan inisialisasikan menjadi np import numpy as np dari library math import function log from math import log dari library sklearn fitur ekstrasi teks import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer fungsi untuk menghitung idf def idf(doc, wordBank): #hitung jumlah doc N = len(doc.index) #buat dataframe dengan header word dan idf result = pd.DataFrame(columns=['word', 'idf']) #untuk setiap kata pada wordBank lakukan..... for index, word in wordBank.iterrows(): #hitung jumlah doc yang mengandung kata word['words'] dft = np.sum(doc.str.contains(word['kata'])) #hitung inverse document frequency smooth idft = log(N / (dft + 1), 10) #tambahkan idf untuk setiap kata pada data frame result = result.append(pd.Series([word['kata'], idft], index=['word', 'idf']), ignore_index=True) #return variable result return result fungsi untuk menghitung tf``def tf(doc, wordBank): #split kata berdasarkan spasi wordList = doc.str.split(' ') #hitung jumlah kata pada setiap doc maxFt = [len(s) for s in wordList] #buat DataFrame kosong untuk menyimpan hasil perhitungan Tf result = pd.DataFrame() #untuk setiap word dalam wordbank lakukan .... for index, word in wordBank.iterrows(): #hitung frekuensi kata untuk setiap doc ft = np.add([s.count(word['kata']) for s in wordList], 0) #tf log normalization ftd = 1 + np.log10(ft+1e-15) #tambahkan hasil perhitungan tf kedalam DataFrame result = result.append(pd.Series(ftd), ignore_index=True) #mengganti -inf dengan 0 result = result.replace(-np.inf, 0) #return variable result return result def tfIdf(tf, idf): #buat DataFrame kosong untuk menyimpan hasil perhitungan Tf-Idf result = pd.DataFrame() #untuk setiap tf for i in tf: #tf idf untuk document term weighting tf * idf tfIdf = tf[i] * idf['idf'] #tambahkan hasil perhitungan tf idf kedalam DataFrame result = result.append(pd.Series(tfIdf), ignore_index=True) #return variable result return result membaca file dataTest.csv menggunakan pandas doc = pd.read_csv('dataTest.csv', encoding='utf-8') membaca file wordBank.csv wordBank.csv berisi kata-kata penting atau keywords yang dianggap relevan dengan topik dalam sebuah kasus vocabulary = pd.read_csv('wordBank.csv', encoding='utf-8') ubah seluruh objek menjadi str/string doc = doc['sentences'].str.lower() mengonversi teks menjadi vektor jumlah kata count = CountVectorizer() bag = count.fit_transform(doc) print(bag) melihat fitur kata a=count.get_feature_names() print(a) hitung nilai IDF resultIDF = idf(doc, vocabulary) print('\\n'+'Hasil perhitungan nilai IDF') print(resultIDF) cetak hasil IDF resultIDF.to_csv(r'nilaiIDF.csv', index = False) print('Output saved in \"nilaiIDF.csv\"') hitung nilai TF resultTF = tf(doc, vocabulary) print('\\n'+'Hasil perhitungan nilai TF') print(resultTF) cetak hasil TF resultTF.to_csv(r'nilaiTF.csv', index = False) print('Output saved in \"nilaiTF.csv\"') hitung nilai Tf-Idf resultTfIdf = tfIdf(resultTF, resultIDF) print('\\n'+'Hasil perhitungan nilai Tf-Idf') print(resultTfIdf) cetak hasil Tf-Idf resultTfIdf.to_csv(r'VSM(TF-IDF).csv', index = False) print('Output saved in \"VSM(TF-IDF).csv\"')","title":"Clusterring"},{"location":"Modelling/#classification","text":"Classification adalah metode yang paling umum pada data mining. Persoalan bisnis sperti Churn Analysis, dan Risk Management biasanya melibatkan metode Classification.Classification adalah tindakan untuk memberikan kelompok pada setiap keadaan. Setiap keadaan berisi sekelompok atribut, salah satunya adalah class attribute. Metode ini butuh untuk menemukan sebuah model yang dapat menjelaskan class attribute itu sebagai fungsi dari input attribute.","title":"Classification"},{"location":"Modelling/#referensi","text":"https://glints.com/id/lowongan/web-crawling-adalah/#.YMFsiKgzbIU","title":"Referensi"},{"location":"Text_Preprocessing/","text":"Text Preprocessing Data teks perlu dibersihkan dan dikodekan ke nilai numerik sebelum diberikan ke model pembelajaran mesin, proses pembersihan dan pengkodean ini disebut sebagai preprocessing teks.Perlu dipersiapkan untuk library ini yakni natural language toolkit atau disingkat nltk yang merupakan library Python dengan pemodelan teks nltk ini juga menyediakan alat yang sebelum digunakan pada mesin learning atau Algo algoritma Deep learning untuk menginstal nltk menggunakan pip pada command Line atau terminal Natural Languange Toolkit (NLKT) Perlu dipersiapkan untuk library ini yakni natural language toolkit atau disingkat nltk yang merupakan library Python dengan pemodelan teks nltk ini juga menyediakan alat yang sebelum digunakan pada mesin learning atau Algo algoritma Deep learning untuk menginstal nltk menggunakan pip pada command Line atau terminal pip install nltk Sebelum menginstal nltk user harus mengunduh paket nltk import nltk nltk.download() Case Folding Text Processing yang sederhana dan efektif yang mempunyai tujuan untuk merubah semua huruf didalam dokumen menjadi huruf kecil abjad huruf a sampai z yang diterima karakter selain objek tersebut dihilangkan dan diangkat delimiter. Mengubah text menjadi lowercase mesin pencarian sangatlah penting contohnya untuk mencari dokumen yang mengandung Indonesia namun tidak ada yang muncul karena indonesia di indeks sebagai INDONESIA. Ini contoh source code Python untuk mengubah teks menjadi lower case kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\" lower_case = kalimat.lower() print(lower_case) #output #berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah korea selatan, jepang, singapura, hong kong, dan finlandia. Menghapus angka Tulisan angka yang tidak relevan dengan apa yang akan dianalisa seperti nomor rumah nomor telepon dan lain-lain regular expression dapat juga digunakan untuk menghapus karakter angkaContoh source code paytren untuk menghapus angka dalam sebuah kalimat import re # impor modul regular expression kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\" hasil = re.sub(r\"\\d+\", \"\", kalimat) print(hasil) # ouput # Berikut ini adalah negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia. Menghapus tanda baca Sama halnya dengan angka, tanda baca dalam kalimat tidak memiliki pengaruh pada text preprocessing. Menghapus tanda baca seperti [!\u201d#$%&\u2019()*+,-./:;<=>?@[]^_`{|}~] dapat dilakukan di pyhton seperti dibawah ini : kalimat = \"Ini &adalah [contoh] kalimat? {dengan} tanda. baca?!!\" hasil = kalimat.translate(str.maketrans(\"\",\"\",string.punctuation)) print(hasil) # output # Ini adalah contoh kalimat dengan tanda baca Menghapus whitepace (karakter kosong) Untuk menghapus spasi di awal dan akhir, anda dapat menggunakan fungsi strip()pada pyhton. Perhatikan kode dibawah ini : kalimat = \" \\t ini kalimat contoh\\t \" hasil = kalimat.strip()print(hasil) # output # ini kalimat contoh Tokenizing Merupakan proses teks menjadi potongan-potongan yang disebut token yang akan dianalisa kata angka simbol dan tanda baca yang memiliki entitas penting dianggap sebagai tokenFungsi split()pada pyhton dapat digunakan untuk memisahkan teks. Perhatikan contoh dibawah ini : kalimat = \"rumah idaman adalah rumah yang bersih\" pisah = kalimat.split() print(pisah) Macam-macam Tokenizing: Tokenizing kata Kalimat atau data dapat dipisahkan menjadi kata-kata yang memiliki kelas word_tokenize() di dalam modul NLTK. # impor word_tokenize dari modul nltk from nltk.tokenize import word_tokenize kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online.\" tokens = nltk.tokenize.word_tokenize(kalimat) print(tokens)# ouput # ['Andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online', '.'] dari output tersebut terdapat kemunculan tanda baca titik dan koma dan juga token \"Andi\" menggunakan huruf kapital. Alangkah lebih baiknya teks harus melewati case folding untuk menghasilkan hasil yang konsisten. salah satu fungsi case folding sebagai penghilang tanda baca serta mengubah teks ke bentuk lowercase. kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower() # output # ['andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online'] Tokenizing kalimat Untuk memisahkan kalimat di dalam pararaf menggunakan sent_tokenize () di dalam modul NLTK, seperti contoh di bawah ini: kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower() # output # ['andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online'] Stopword Stopword merupakan kata umum yang sering muncul dalam jumlah yang besar dan tidak mempunyai makna. Contoh stopword dalam bahasa Indonesia adalah \u201cyang\u201d, \u201cdan\u201d, \u201cdi\u201d, \u201cdari\u201d, dll. from nltk.tokenize import sent_tokenize, word_tokenize from nltk.corpus import stopwords kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\" kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower() tokens = word_tokenize(kalimat) listStopword = set(stopwords.words('indonesian')) removed = [] for t in tokens: if t not in listStopword: removed.append(t) print(removed)# ouput # ['andi', 'kerap', 'transaksi', 'rutin', 'daring', 'online', 'andi', 'belanja', 'online', 'praktis', 'murah'] Stemming Stemming adalah proses menghilangkan tatanan kata ke bentuk dasar semulanya. contohnya \"membaca\", \"membacakan\" akan diubah menjadi \"baca\". Stemming dengan NLTK (bahasa inggris) Algoritma stemming yang sering digunakan adalah PorterStemmer() from nltk.stem import PorterStemmer ps = PorterStemmer() kata = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] for k in kata: print(k, \" : \", ps.stem(k))# ouput # program : program programs : program programer : program programing : program programers : program Stemming bahasa Indonesia menggunakan Python Sastrawi Di dalam teks berbahasa inggris, proses yang paling penting yaitu penghilangan sufiks. Di dalam bahasa Indonesia kata imbuhan sufiks dan prefiks dihilangkan juga. from Sastrawi.Stemmer.StemmerFactory import StemmerFactoryfactory = StemmerFactory() stemmer = factory.create_stemmer() kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"hasil = stemmer.stem(kalimat)print(hasil)# ouput # andi kerap laku transaksi rutin cara daring atau online turut andi belanja online lebih praktis murah Source Code Tugas Kuliah Web Mining import pandas as pd import re import csv import string df = pd.read_csv('dataTrain.csv', encoding='utf-8') df = df['sentences'].str.lower() emoticons_str = r\"\"\" (?: [:=;] # Eyes [oO-]? # Nose (optional) [D)](]/\\OpP] # Mouth )\"\"\" regex_str = [ emoticons_str, r'<[^>]+>', # HTML tags r'(?:@[\\w_]+)', # @-mentions r\"(?:#+[\\w_]+[\\w\\'_-] [\\w_]+)\", # hash-tags r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[! (),]|(?:%[0-9a-f][0-9a-f]))+', # URLs r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers r\"(?:[a-z][a-z'\\-\\_]+[a-z])\", # words with - and ' r'(?:[\\w_]+)', # other words r'(?:\\S)' # anything else ] tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE) emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE) def tokenize(s): return tokens_re.findall(s) def preproses(s, lowercase=False): tokens = tokenize(s) filtered_words = [w for w in tokens] filt = \" \".join(filtered_words) filt = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', filt, flags=re.MULTILINE) # URLs filt = re.sub(r'@[\\w_]+','', filt) # @-mentions filt = re.sub(r\"(?:#+[\\w_]+[\\w\\' -]*[\\w ]+)\", '', filt) #hash-tags filt = re.sub(r\"\\d+\", \"\", filt) # angka filt = filt.translate(str.maketrans('','',string.punctuation)) # hapus tanda baca return filt def removeNonWord(doc): result = doc.replace({'[\\W_]+': ' '}, regex=True) return result[result.notnull()] def writeToCsv(): count_row = df.shape[0] with open('dataTest.csv', 'w', encoding='utf-8') as file: w = csv.writer(file) w.writerow(['sentences']) #menulis baris header for i in range(count_row): x = preproses(df.loc[i]) x = x.strip() w.writerow([x]) writeToCsv() doc = pd.read_csv('dataTest.csv', encoding='utf-8') doc = doc['sentences'].str.lower() doc = removeNonWord(doc) doc.to_csv(r'dataTest.csv', index = False) print('Output saved in \"dataTest.csv\"') Referensi Adriani, M., Asian, J., Nazief, B., Tahaghoghi, S. M. M., & Williams, H. E. (2007). Stemming Indonesian. ACM Transactions on Asian Language Information Processing, 6(4), 1\u201333. Tala, Fadillah Z.(1999). A Study of Stemming Effect on Information Retrieval in Bahasa Indonesia. Geovedi, Jim.(2014).Karena Data Gak Mungkin Bohong dan karena Bisa Diolah Sesuai Pesanan ( https://medium.com/curahan-rekanalar/karena-data-gak-mungkin-bohong-a17ff90cef87 ). Python Sastrawi ( https://github.com/har07/PySastrawi ). Natural Language Toolkit -NLTK ( https://www.nltk.org ). Matplotlib ( https://matplotlib.org ).","title":"**Text Preprocessing**"},{"location":"Text_Preprocessing/#text-preprocessing","text":"Data teks perlu dibersihkan dan dikodekan ke nilai numerik sebelum diberikan ke model pembelajaran mesin, proses pembersihan dan pengkodean ini disebut sebagai preprocessing teks.Perlu dipersiapkan untuk library ini yakni natural language toolkit atau disingkat nltk yang merupakan library Python dengan pemodelan teks nltk ini juga menyediakan alat yang sebelum digunakan pada mesin learning atau Algo algoritma Deep learning untuk menginstal nltk menggunakan pip pada command Line atau terminal","title":"Text Preprocessing"},{"location":"Text_Preprocessing/#natural-languange-toolkit-nlkt","text":"Perlu dipersiapkan untuk library ini yakni natural language toolkit atau disingkat nltk yang merupakan library Python dengan pemodelan teks nltk ini juga menyediakan alat yang sebelum digunakan pada mesin learning atau Algo algoritma Deep learning untuk menginstal nltk menggunakan pip pada command Line atau terminal pip install nltk Sebelum menginstal nltk user harus mengunduh paket nltk import nltk nltk.download()","title":"Natural Languange Toolkit (NLKT)"},{"location":"Text_Preprocessing/#case-folding","text":"Text Processing yang sederhana dan efektif yang mempunyai tujuan untuk merubah semua huruf didalam dokumen menjadi huruf kecil abjad huruf a sampai z yang diterima karakter selain objek tersebut dihilangkan dan diangkat delimiter.","title":"Case Folding"},{"location":"Text_Preprocessing/#mengubah-text-menjadi-lowercase","text":"mesin pencarian sangatlah penting contohnya untuk mencari dokumen yang mengandung Indonesia namun tidak ada yang muncul karena indonesia di indeks sebagai INDONESIA. Ini contoh source code Python untuk mengubah teks menjadi lower case kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\" lower_case = kalimat.lower() print(lower_case) #output #berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah korea selatan, jepang, singapura, hong kong, dan finlandia.","title":"Mengubah text menjadi lowercase"},{"location":"Text_Preprocessing/#menghapus-angka","text":"Tulisan angka yang tidak relevan dengan apa yang akan dianalisa seperti nomor rumah nomor telepon dan lain-lain regular expression dapat juga digunakan untuk menghapus karakter angkaContoh source code paytren untuk menghapus angka dalam sebuah kalimat import re # impor modul regular expression kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\" hasil = re.sub(r\"\\d+\", \"\", kalimat) print(hasil) # ouput # Berikut ini adalah negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.","title":"Menghapus angka"},{"location":"Text_Preprocessing/#menghapus-tanda-baca","text":"Sama halnya dengan angka, tanda baca dalam kalimat tidak memiliki pengaruh pada text preprocessing. Menghapus tanda baca seperti [!\u201d#$%&\u2019()*+,-./:;<=>?@[]^_`{|}~] dapat dilakukan di pyhton seperti dibawah ini : kalimat = \"Ini &adalah [contoh] kalimat? {dengan} tanda. baca?!!\" hasil = kalimat.translate(str.maketrans(\"\",\"\",string.punctuation)) print(hasil) # output # Ini adalah contoh kalimat dengan tanda baca","title":"Menghapus tanda baca"},{"location":"Text_Preprocessing/#menghapus-whitepace-karakter-kosong","text":"Untuk menghapus spasi di awal dan akhir, anda dapat menggunakan fungsi strip()pada pyhton. Perhatikan kode dibawah ini : kalimat = \" \\t ini kalimat contoh\\t \" hasil = kalimat.strip()print(hasil) # output # ini kalimat contoh","title":"Menghapus whitepace (karakter kosong)"},{"location":"Text_Preprocessing/#tokenizing","text":"Merupakan proses teks menjadi potongan-potongan yang disebut token yang akan dianalisa kata angka simbol dan tanda baca yang memiliki entitas penting dianggap sebagai tokenFungsi split()pada pyhton dapat digunakan untuk memisahkan teks. Perhatikan contoh dibawah ini : kalimat = \"rumah idaman adalah rumah yang bersih\" pisah = kalimat.split() print(pisah) Macam-macam Tokenizing:","title":"Tokenizing"},{"location":"Text_Preprocessing/#tokenizing-kata","text":"Kalimat atau data dapat dipisahkan menjadi kata-kata yang memiliki kelas word_tokenize() di dalam modul NLTK. # impor word_tokenize dari modul nltk from nltk.tokenize import word_tokenize kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online.\" tokens = nltk.tokenize.word_tokenize(kalimat) print(tokens)# ouput # ['Andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online', '.'] dari output tersebut terdapat kemunculan tanda baca titik dan koma dan juga token \"Andi\" menggunakan huruf kapital. Alangkah lebih baiknya teks harus melewati case folding untuk menghasilkan hasil yang konsisten. salah satu fungsi case folding sebagai penghilang tanda baca serta mengubah teks ke bentuk lowercase. kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower() # output # ['andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online']","title":"Tokenizing kata"},{"location":"Text_Preprocessing/#tokenizing-kalimat","text":"Untuk memisahkan kalimat di dalam pararaf menggunakan sent_tokenize () di dalam modul NLTK, seperti contoh di bawah ini: kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower() # output # ['andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online']","title":"Tokenizing kalimat"},{"location":"Text_Preprocessing/#stopword","text":"Stopword merupakan kata umum yang sering muncul dalam jumlah yang besar dan tidak mempunyai makna. Contoh stopword dalam bahasa Indonesia adalah \u201cyang\u201d, \u201cdan\u201d, \u201cdi\u201d, \u201cdari\u201d, dll. from nltk.tokenize import sent_tokenize, word_tokenize from nltk.corpus import stopwords kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\" kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower() tokens = word_tokenize(kalimat) listStopword = set(stopwords.words('indonesian')) removed = [] for t in tokens: if t not in listStopword: removed.append(t) print(removed)# ouput # ['andi', 'kerap', 'transaksi', 'rutin', 'daring', 'online', 'andi', 'belanja', 'online', 'praktis', 'murah']","title":"Stopword"},{"location":"Text_Preprocessing/#stemming","text":"Stemming adalah proses menghilangkan tatanan kata ke bentuk dasar semulanya. contohnya \"membaca\", \"membacakan\" akan diubah menjadi \"baca\".","title":"Stemming"},{"location":"Text_Preprocessing/#stemming-dengan-nltk-bahasa-inggris","text":"Algoritma stemming yang sering digunakan adalah PorterStemmer() from nltk.stem import PorterStemmer ps = PorterStemmer() kata = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] for k in kata: print(k, \" : \", ps.stem(k))# ouput # program : program programs : program programer : program programing : program programers : program","title":"Stemming dengan NLTK (bahasa inggris)"},{"location":"Text_Preprocessing/#stemming-bahasa-indonesia-menggunakan-python-sastrawi","text":"Di dalam teks berbahasa inggris, proses yang paling penting yaitu penghilangan sufiks. Di dalam bahasa Indonesia kata imbuhan sufiks dan prefiks dihilangkan juga. from Sastrawi.Stemmer.StemmerFactory import StemmerFactoryfactory = StemmerFactory() stemmer = factory.create_stemmer() kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"hasil = stemmer.stem(kalimat)print(hasil)# ouput # andi kerap laku transaksi rutin cara daring atau online turut andi belanja online lebih praktis murah Source Code Tugas Kuliah Web Mining import pandas as pd import re import csv import string df = pd.read_csv('dataTrain.csv', encoding='utf-8') df = df['sentences'].str.lower() emoticons_str = r\"\"\" (?: [:=;] # Eyes [oO-]? # Nose (optional) [D)](]/\\OpP] # Mouth )\"\"\" regex_str = [ emoticons_str, r'<[^>]+>', # HTML tags r'(?:@[\\w_]+)', # @-mentions r\"(?:#+[\\w_]+[\\w\\'_-] [\\w_]+)\", # hash-tags r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[! (),]|(?:%[0-9a-f][0-9a-f]))+', # URLs r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers r\"(?:[a-z][a-z'\\-\\_]+[a-z])\", # words with - and ' r'(?:[\\w_]+)', # other words r'(?:\\S)' # anything else ] tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE) emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE) def tokenize(s): return tokens_re.findall(s) def preproses(s, lowercase=False): tokens = tokenize(s) filtered_words = [w for w in tokens] filt = \" \".join(filtered_words) filt = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', filt, flags=re.MULTILINE) # URLs filt = re.sub(r'@[\\w_]+','', filt) # @-mentions filt = re.sub(r\"(?:#+[\\w_]+[\\w\\' -]*[\\w ]+)\", '', filt) #hash-tags filt = re.sub(r\"\\d+\", \"\", filt) # angka filt = filt.translate(str.maketrans('','',string.punctuation)) # hapus tanda baca return filt def removeNonWord(doc): result = doc.replace({'[\\W_]+': ' '}, regex=True) return result[result.notnull()] def writeToCsv(): count_row = df.shape[0] with open('dataTest.csv', 'w', encoding='utf-8') as file: w = csv.writer(file) w.writerow(['sentences']) #menulis baris header for i in range(count_row): x = preproses(df.loc[i]) x = x.strip() w.writerow([x]) writeToCsv() doc = pd.read_csv('dataTest.csv', encoding='utf-8') doc = doc['sentences'].str.lower() doc = removeNonWord(doc) doc.to_csv(r'dataTest.csv', index = False) print('Output saved in \"dataTest.csv\"')","title":"Stemming bahasa Indonesia menggunakan Python Sastrawi"},{"location":"Text_Preprocessing/#referensi","text":"Adriani, M., Asian, J., Nazief, B., Tahaghoghi, S. M. M., & Williams, H. E. (2007). Stemming Indonesian. ACM Transactions on Asian Language Information Processing, 6(4), 1\u201333. Tala, Fadillah Z.(1999). A Study of Stemming Effect on Information Retrieval in Bahasa Indonesia. Geovedi, Jim.(2014).Karena Data Gak Mungkin Bohong dan karena Bisa Diolah Sesuai Pesanan ( https://medium.com/curahan-rekanalar/karena-data-gak-mungkin-bohong-a17ff90cef87 ). Python Sastrawi ( https://github.com/har07/PySastrawi ). Natural Language Toolkit -NLTK ( https://www.nltk.org ). Matplotlib ( https://matplotlib.org ).","title":"Referensi"},{"location":"Web_Crawling/","text":"Web Crawling Crawling adalah proses mesin pencari untuk menemukan halaman website gambar video dokumen dan lain sebagainya yang telah di update di sebuah situs.Untuk pencarian sebuah konten di search engine dengan keyword tertentu maka akan mencari indeks dan konten yang mana yang paling sesuai untuk user tersebut.Ada banyak beragam pilihan tools yang dapat digunakan untuk melakukan web Crawling salah satunya disebut sebagai web robot atau web Spider dan juga proses web rolling ini tidak dapat dilakukan secara manual cara kerja Web Crawler Web crawler akan menuju ke laman situs dan link Masukkan URL situs di Google Search Console Web Crawling akan melakukan pencatatan pada setiap link di indeks.note : halaman/link yang bersifat privat tidak bisa diambil Informasinyainformasi yang sudah terumpul maka akan di simpan di dalam indeks search engine sehingga muncul di konten dengan keyword yang sama Contoh Web Crawler Googlebot, web crawler milik Google yang paling banyak digunakan saat ini. Gogglebot akan membuat indeks yang akan bertugas untuk mengumpulkan informasi dari berbagai website. HTTrack, web crawler yang bersifat open source. ketika user sudah mendownload aplikasi ini maka user bia membuka konten situs tanpa melalui koneksi internet Cyotek WebCopy, sama halnya seperti HTTrack yang membedakan user dapat memilih bagian mana yang ingin di download. Webhose, web crawler yang mengubah konten menjadi datafeeds Source Code import scrapy class ReviewSpider(scrapy.Spider): name = 'review' allowed_domains = ['jawapos.com'] start_urls = ['https://www.jawapos.com/berita-hari-ini/'] def parse(self, response): data = response.css('.post-list__container') # Collecting title title = data.css('.post-list__title') # Collecting price cat = data.css('.post-list__cat') c=0 time = data.css('.post-list__time') # Combining the results for review in title: yield{'title': ''.join(review.xpath('.//text()').extract()), 'cat': ''.join(cat[c].xpath(\".//text()\").extract()), 'time': ''.join(time[c].xpath(\"./text()\").extract()) } c=c+1 Referensi https://glints.com/id/lowongan/web-crawling-adalah/#.YMIc0fkzbIV","title":"Web Crawling"},{"location":"Web_Crawling/#web-crawling","text":"Crawling adalah proses mesin pencari untuk menemukan halaman website gambar video dokumen dan lain sebagainya yang telah di update di sebuah situs.Untuk pencarian sebuah konten di search engine dengan keyword tertentu maka akan mencari indeks dan konten yang mana yang paling sesuai untuk user tersebut.Ada banyak beragam pilihan tools yang dapat digunakan untuk melakukan web Crawling salah satunya disebut sebagai web robot atau web Spider dan juga proses web rolling ini tidak dapat dilakukan secara manual","title":"Web Crawling"},{"location":"Web_Crawling/#cara-kerja-web-crawler","text":"Web crawler akan menuju ke laman situs dan link Masukkan URL situs di Google Search Console Web Crawling akan melakukan pencatatan pada setiap link di indeks.note : halaman/link yang bersifat privat tidak bisa diambil Informasinyainformasi yang sudah terumpul maka akan di simpan di dalam indeks search engine sehingga muncul di konten dengan keyword yang sama","title":"cara kerja Web Crawler"},{"location":"Web_Crawling/#contoh-web-crawler","text":"Googlebot, web crawler milik Google yang paling banyak digunakan saat ini. Gogglebot akan membuat indeks yang akan bertugas untuk mengumpulkan informasi dari berbagai website. HTTrack, web crawler yang bersifat open source. ketika user sudah mendownload aplikasi ini maka user bia membuka konten situs tanpa melalui koneksi internet Cyotek WebCopy, sama halnya seperti HTTrack yang membedakan user dapat memilih bagian mana yang ingin di download. Webhose, web crawler yang mengubah konten menjadi datafeeds Source Code import scrapy class ReviewSpider(scrapy.Spider): name = 'review' allowed_domains = ['jawapos.com'] start_urls = ['https://www.jawapos.com/berita-hari-ini/'] def parse(self, response): data = response.css('.post-list__container') # Collecting title title = data.css('.post-list__title') # Collecting price cat = data.css('.post-list__cat') c=0 time = data.css('.post-list__time') # Combining the results for review in title: yield{'title': ''.join(review.xpath('.//text()').extract()), 'cat': ''.join(cat[c].xpath(\".//text()\").extract()), 'time': ''.join(time[c].xpath(\"./text()\").extract()) } c=c+1","title":"Contoh Web Crawler"},{"location":"Web_Crawling/#referensi","text":"https://glints.com/id/lowongan/web-crawling-adalah/#.YMIc0fkzbIV","title":"Referensi"},{"location":"Web_Crawling/#_1","text":"","title":""}]}